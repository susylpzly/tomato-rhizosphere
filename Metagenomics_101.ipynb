{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metagenomics 101 course\n",
    "- https://staff.fnwi.uva.nl/a.u.s.heintzbuschart/metagenomics_00.html  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 1  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- lecture 1: https://video.uva.nl/media/Metagenomics+101+-+1%7C+overview+-+private+version/0_pvgszh0o \n",
    "    - definition of metagenomics\n",
    "    - brief history of metagenomics\n",
    "    - glimpse into raw data\n",
    "    - typical steps in an analysis\n",
    "- Tutorial 1:\n",
    "    - Set up: didn't initialize miniconda since it had already been used before and saved data in personal directory.\n",
    "        - First test-run of IMP3 in Crunchomics - dry run to test configuration_\n",
    "            - /zfs/omics/projects/metatools/TOOLS/IMP3/runIMP3 -d my.config.yaml\n",
    "            - desired output: This was a dry-run (flag -n). The order of jobs does __not__ reflect the order of execution.\n",
    "        - Submitted the first test run to the compute nodes of crunchomics server in node 3 with omics-cn003.\n",
    "            - first check the available nodes: sinfo -o \"%n %e %m %a %c %C\" \n",
    "        - IMP3 inputs: data and configuration file \n",
    "            - my.config.yaml configuration file is the only required argument and specifies the input data. \n",
    "            - code used to commit the real run to the compute node, which includes more arguments than the previous dry run: \n",
    "                - /zfs/omics/projects/metatools/TOOLS/IMP3/runIMP3 -c -r -n TESTRUN -b omics-cn002 my.config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lecture 2: Raw Data Preprocessing and QC \n",
    "    - .fastq files tools resource: http://barcwiki.wi.mit.edu/wiki/SOPs/qc_shortReads \n",
    "    - brief look into different sequencing techniques\n",
    "    - what can go wrong when sequencing\n",
    "    - sequencing quality scores\n",
    "    - quality reports\n",
    "    - adapter, spike-in, and host genome removal (the host microbiome needs to be removed because you are interested in the other genomes in the sample)\n",
    "- Tutorial 2: Run Preprocessing on Toy Data\n",
    "    - Finding Novaseq file in reads data - the GCEF.test.r1.fq and GCEF.test.r2.fq are novaseq\n",
    "    - code used to modify the configuration file __prep.1.config.yaml__ to use datasets COSMIC_vag.test.r1.fq and COSMIC_vag.test.r2.fq instead of the default test data, by manually changing inside the configuration file by using nano command: \n",
    "        - raws:\n",
    "            Metagenomics: \"/zfs/omics/projects/metatools/SANDBOX/Metagenomics101/EXAMPLE_DATA/READS/COSMIC_vag.test.r1.fq /zfs/omics/projects/metatools/SANDBOX/Metagenomics101/EXAMPLE_DATA/READS/COSMIC_vag.test.r2.fq\"\n",
    "    - made a copy of prep.1.config.yaml to be able to run both configuration at the same time and have their own output directories. \n",
    "    - code used to modify the configuration file __prep.2.config.yaml__ to use datasets GCEF.test.r1.fq and GCEF.test.r2.fq instead of the default test data, by manually changing inside the configuration file by using __nano__ command: \n",
    "        - raws:\n",
    "            Metagenomics: \"/zfs/omics/projects/metatools/SANDBOX/Metagenomics101/EXAMPLE_DATA/READS/GCEF.test.r1.fq /zfs/omics/projects/metatools/SANDBOX/Metagenomics101/EXAMPLE_DATA/READS/GCEF.test.r2.fq\"\n",
    "        - also changed the output directory to __IMP3_test_preprocessing_novaseq__, and nextseq argument to __true__. \n",
    "    - performed the dry run and obtained desired output:\n",
    "        - /zfs/omics/projects/metatools/TOOLS/IMP3/runIMP3 -d prep.1.config.yaml \n",
    "        - /zfs/omics/projects/metatools/TOOLS/IMP3/runIMP3 -d prep.2.config.yaml \n",
    "    - both runs were commited to node 2:\n",
    "            - /zfs/omics/projects/metatools/TOOLS/IMP3/runIMP3 -c -r -n TESTRUN -b omics-cn002 prep.1.config.yaml\n",
    "            - /zfs/omics/projects/metatools/TOOLS/IMP3/runIMP3 -c -r -n TESTRUN -b omics-cn002 prep.2.config.yaml\n",
    "    - The dataset COSMIC_vag.test.r1|2.fq comes from a sample that contains still host genomics DNA\n",
    "        - prep.1.config.yaml was copied to prep.3.config.yaml and a new configuration was specified with the same test data, a new output directory __IMP3_test_preprocessing_hg38filtering__, and the filtering was specified to hg38 to filter out the human genome \n",
    "        - dry run was performed and successful \n",
    "        - run was submitted to node 4: \n",
    "            - /zfs/omics/projects/metatools/TOOLS/IMP3/runIMP3 -c -r -n TESTRUN -b omics-cn004 prep.3.config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lecture 3: Mapping Metagenomics read data to reference genomes \n",
    "    - why mapping\n",
    "    - bioinformatics for sequence alignment\n",
    "    - look at output (alignments) created by read mappers\n",
    "    - what can be done with the output  \n",
    "- Tutorial 3:\n",
    "    - not IMP3 pipeline but the commands on their own  \n",
    "    - environments IMP3 provides on crunchomics to preprocess data with BWA-MEM - Burrows Wheeler algoritm (transform) - Maximum Exact Matches \n",
    "    1. Running BWA-MEM indexing \n",
    "        - build index of references (don't do on the frontend if using a real genomes, okay now bc test data)\n",
    "        - cp /zfs/omics/projects/metatools/SANDBOX/Metagenomics101/EXAMPLE_DATA/REFERENCES/mg.assembly.merged.fa . (copied to my personal directory)\n",
    "        - conda activate /zfs/omics/projects/metatools/TOOLS/IMP3/conda/76e02d85877600c41ac84cb7bc27a189\n",
    "        - bwa index mg.assembly.merged.fa \n",
    "    2. Running mapping with BWA-MEM \n",
    "        - bwa mem -v 1 -t 1  mg.assembly.merged.fa /zfs/omics/projects/metatools/SANDBOX/Metagenomics101/EXAMPLE_DATA/READS/test.mg.1.fastq /zfs/omics/projects/metatools/SANDBOX/Metagenomics101/EXAMPLE_DATA/READS/test.mg.2.fastq >> test_alignment.sam\n",
    "        - you can run other data, for examble: COMSIC_vag.test.r1|r2.fq with the hg38.fa - first need to find hg38.fa file to copy to personal directory and index it \n",
    "    3. Take a look at the alignment file \n",
    "        - output of BWA-MEM is an alignment file in .sam format. Alignment information is at the end of the file \n",
    "        - tail test_alignment.sam \n",
    "    4. Converting the alignment file to a binary (compressed) format \n",
    "        - convert to .bam format because the .sam is very big and you don't want to store on own disk \n",
    "        - samtools view --threads 1 -bS test_alignment.sam > test_alignment.bam\n",
    "        - can also be donde in one step with the mapping:                 - bwa mem -v 1 -t 1  /zfs/omics/projects/metatools/DB/filtering/hg38.fa /zfs/omics/projects/metatools/SANDBOX/Metagenomics101/EXAMPLE_DATA/READS/COSMIC_vag.test.r1.fq /zfs/omics/projects/metatools/SANDBOX/Metagenomics101/EXAMPLE_DATA/READS/COSMIC_vag.test.r2.fq |\\\n",
    "                    samtools view --threads 1 -bS - > COSMIC_alignment.bam\n",
    "        - can look at a .bam file with samtools view: samtools view test_alignment.bam | less\n",
    "    5. Filtering the .bam file \n",
    "        - also using samtools view, use -f 4 to find only reads which don't map to reference \n",
    "        - samtools view -f 4 test_alignment.bam | less \n",
    "        - this shows many of the values to be 0 \n",
    "        - -F 4 give reads which map (careful, there's also a second read)\n",
    "        - more samtools flags: https://broadinstitute.github.io/picard/explain-flags.html\n",
    "        - samtools manual: http://www.htslib.org/doc/samtools-view.html \n",
    "    6. More complex filtering \n",
    "        - used in IMP3 to remove reads mapping to a host genome \n",
    "        - samtools merge --threads 1 -u - \\\n",
    "            <(samtools view --threads 1 -u  -f 4 -F 264 COSMIC_alignment.bam) \\\n",
    "            <(samtools view --threads 1 -u -f 8 -F 260 COSMIC_alignment.bam) \\\n",
    "            <(samtools view --threads 1 -u -f 12 -F 256 COSMIC_alignment.bam)  | \\\n",
    "            samtools view --threads 1 -bF 0x800 - | \\\n",
    "            samtools sort --threads 1 -m 3G -n - | \\\n",
    "            bamToFastq -i stdin -fq COSMIC_filtered.r1.fq -fq2 COSMIC_filtered.r2.fq\n",
    "\n",
    "            conda deactivate \n",
    "            (all COSMIC_alignment.bam files are the test_alignment.bam files when doing the alignment of COSMIC files to the hg38.fa reference genome - didn't actually perform that alignment) (figure out what the code on top is doing by looking at the resources of flags and samtools documentation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lecture 4: Taxonomy\n",
    "    - different approaches toprofile microbiomes in terms of the present taxa, using metagenomics read data \n",
    "    - introduction into the aims and particular environments \n",
    "    - a touch of taxonomy \n",
    "    - brief reminder of mapping of metagenomis reads \n",
    "    - look at non-alignment based profilers \n",
    "    - how taxonomic profilers are benchmarked \n",
    "    - what we can do with this output \n",
    "- Tutorial 4: Run mOTUs2, MetaPhlan2, and kraken2\n",
    "    - Use the taxonomic profiles in the IMP3 pipeline as well as MetaPhlan3 (IMP3 in crunchomics and metaphlan3 conda environment)\n",
    "    - use cd /zfs/omics/projects/metatools/SANDBOX/Metagenomics101/EXAMPLE_DATA/READS files or the preprocessed files from the last sessions\n",
    "    - Running taxonomic profiling within __IMP3__ \n",
    "        - start IMP3 with an appropriate configuration file since data is preprocessed. Set config file to not do preprocessing, and set input as already preprocessed files\n",
    "        > cp /zfs/omics/projects/metatools/SANDBOX/Metagenomics101/04_taxonomy1/tax.config.yaml my.tax.config.yaml \n",
    "        - change (using nano) to input own data. Can also change the kraken databse to one of the names in /zfs/omics/projects/metatools/DB with the word kraken in it (UHGP_kraken2 for human gut genomes, minikraken2 for an even smaller database; kraken_pfp8 is the current kraken db in configuration). Output directory: test_tax_output\n",
    "            >raws:\n",
    "            >>Metagenomics: /home/14108550/personal/IMP3_test_preprocessing/Preprocessing/mg.r1.trimmed.phiX174_filtered.fq\n",
    "            >>/home/14108550/personal/IMP3_test_preprocessing/Preprocessing/mg.r2.trimmed.phiX174_filtered.fq\n",
    "            >>/home/14108550/personal/IMP3_test_preprocessing/Preprocessing/mg.se.trimmed.phiX174_filtered.fq\n",
    "        - first did dry run, which was successful \n",
    "            > /zfs/omics/projects/metatools/TOOLS/IMP3/runIMP3 -d my.tax.config.yaml\n",
    "        - then commited run to node 3 \n",
    "            > /zfs/omics/projects/metatools/TOOLS/IMP3/runIMP3 -c -r -n TESTTAX -b omics-cn003 my.tax.config.yaml\n",
    "        - inside the output directory will be another directory Analysis/taxonomy which holds the outputs from mOTUs2 and kraken2 \n",
    "        - Taxonomic profiling using __MetaPhlan3__ \n",
    "            - works very well on human samples and has a strain-level module. \n",
    "            - activate the conda environment: \n",
    "            > conda activate /zfs/omics/projects/metatools/TOOLS/miniconda3/envs/metaphlan-3.0\n",
    "            - to get help and documentation on the profiler:\n",
    "            > metaphlan -h \n",
    "            - example to run on filtered test reads:\n",
    "            > metaphlan /zfs/omics/projects/metatools/SANDBOX/Metagenomics101/EXAMPLE_DATA/FILTERED_READS/mg.r1.trimmed.phiX174_filtered.fq,/zfs/omics/projects/metatools/SANDBOX/Metagenomics101/EXAMPLE_DATA/FILTERED_READS/mg.r2.trimmed.phiX174_filtered.fq,/zfs/omics/projects/metatools/SANDBOX/Metagenomics101/EXAMPLE_DATA/FILTERED_READS/mg.se.trimmed.phiX174_filtered.fq --bowtie2out filtered_test.bowtie2.bz2 --input_type fastq -o profile.filtered_test.txt  --bowtie2db /zfs/omics/projects/metatools/DB/metaphlan --unknown_estimation\n",
    "            - can also run on own filtered reads instead. to run on real data set, put the commands to activate (and deactivate) conda and the metaphlan command into a script that you submit to the cluster. You can then also choose to set the --nproc argument (--nproc 8 to parallelize the profiling) \n",
    "            - code for runnin the non-filtered reads \n",
    "            > metaphlan /zfs/omics/projects/metatools/SANDBOX/Metagenomics101/EXAMPLE_DATA/READS/test.mg.r1.fastq,/zfs/omics/projects/metatools/SANDBOX/Metagenomics101/EXAMPLE_DATA/READS/test.mg.r2.fastq --bowtie2out unfiltered_test.bowtie2.bz2 --input_type fastq -o profile.unfiltered_test.txt  --bowtie2db /zfs/omics/projects/metatools/DB/metaphlan --unknown_estimation\n",
    "            - To merge the profiles from several samples (or in this case, the same sample, but represented by filtered or unfiltered reads), you can use metaphlanâ€™s merge script\n",
    "            > merge_metaphlan_tables.py profile*filtered_test.txt > merged_abundance.test.tsv\n",
    "            - to compare the results:\n",
    "            > less merged_abundance.test.tsv\n",
    "            > conda deactivate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Metagenomics 101 Lecture 5: Assembly\n",
    "    - lecture video: https://video.uva.nl/media/Metagenomics+101+-+5%7C+Assembly+-+private+version/0_p8gnde38 \n",
    "    - why assemble\n",
    "    - alternatives to assembly\n",
    "    - how does assembly work\n",
    "    - how to inspect assemblies\n",
    "    - what to assemble \n",
    "    - stopped at min 17 \n",
    "- Tutorial 5: Run assemblies in IMP3 \n",
    "    - assemble with IMP3 and look at diagnostic plots "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Metagenomics Lecture 6: Metagenome Annotation (of assembled contigs)\n",
    "    - finding bacterial genes\n",
    "        - protein coding genes\n",
    "        - rRNAs\n",
    "    - annotating genes with functions\n",
    "        - why not just align?\n",
    "        - HMMs and HMMER\n",
    "- Tutorial 6: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Metagenomics 101 Lecture 7 \n",
    "  - approaches to make (semi-)quantitative profiles of the molecular functions found in metagenomes and how they can be compared across samples\n",
    "  - review on mapping and annotation\n",
    "  - gene abundance measures - functional profile calculation:\n",
    "    - reads per gene/reads per function\n",
    "    - reads per kilobase\n",
    "    - copies per million\n",
    "    - average depth of coverage\n",
    "\n",
    "- Tutorial: take a look at annotations:\n",
    "  - bash tricks to get a quick look at annotations: \n",
    "    - head annotation_CDS_RNA_hmms.gff\n",
    "    - tail -n 5 annotation_CDS_RNA_hmms.gff\n",
    "    - wc -l annotation_CDS_RNA_hmms.gff --> gives a count of annotated sequences \n",
    "      - wc stands for word count \n",
    "      - -l option prints the number of lines present in a file \n",
    "    -  grep \"CDS\" -c annotation_CDS_RNA_hmms.gff --> 8382 is the number of lines that are CDS type \n",
    "      - grep is a command-line utility for searching plain-text data sets for lines that match a regular expression\n",
    "      - -c flag is used to ask for the total number of lines that contain that string - counts \n",
    "    - cut -f 3 annotation_CDS_RNA_hmms.gff | sort | uniq -c\n",
    "      - what field are in the column 3? \n",
    "      - 8382 CDS\n",
    "      - 1 repeat_region\n",
    "      - 17 rRNA\n",
    "      - 1 tmRNA\n",
    "      - 63 tRNA\n",
    "    - grep \"   CDS     \" -v annotation_CDS_RNA_hmms.gff | cut -f 3 | sort | uniq -c\n",
    "      - yield the same information as the above script except without the count for CDS \n",
    "    - grep \"kegg_ko\" -c annotation_CDS_RNA_hmms.gff\n",
    "      - how many sequences are annotated with a KEGG KO? 4031 \n",
    "    - grep \"kegg_ko\" annotation_CDS_RNA_hmms.gff | sed 's#.*kegg_ko=\\([^;]*\\).*#\\1#' | tr ',' '\\n' | sort | uniq -c | sort -n\n",
    "      - which KO is most common? \n",
    "    - Try to adapt the code above to the Pfam domains:\n",
    "      - grep \"pfam\" annotation_CDS_RNA_hmms.gff | sed 's#.*pfam=\\([^;]*\\).*#\\1#' | tr ',' '\\n' | sort | uniq -c | sort -n\n",
    "    - Try to find the cog annotations of the genes are annotated with the most common KO\n",
    "      - grep \"kegg_ko\" annotation_CDS_RNA_hmms.gff | sed 's#.*cog=\\([^;]*\\).*#\\1#' | tr ',' '\\n' | sort | uniq -c | sort -n \n",
    "    - Try to find the Pfam annotations of the genes are annotated with the most common KO. Check the Pfam DB and KEGG DB for what these annotations mean. Do they correspond to a similar function?\n",
    "      - grep \"kegg_ko\" annotation_CDS_RNA_hmms.gff | sed 's#.*pfam=\\([^;]*\\).*#\\1#' | tr ',' '\\n' | sort | uniq -c | sort -n\n",
    "      - search the databases manually? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Metagenomics 101: Lecture 8: Functional Profiles\n",
    "    - approaches to make (semi-)quantitative profiles of the molecular functions found in metagenomes, and how they can be compared across sambles\n",
    "    - look back at mapping and annotation\n",
    "    - Gene abundance measures - functional profile calculation\n",
    "        - reads per gene/reads per functin\n",
    "        - reads per kilobase\n",
    "        - copies per million\n",
    "        - average depth of coverage\n",
    "    - working with functional profiles \n",
    "- Tutorial: counting reads and calculating profiles\n",
    "    - see how IMP3 calculates functional profiles and the depth of coverage of genes \n",
    "    - files used: annotation_CDS_RNA_hmms.gff  \n",
    "        - from annotation on Analysis/annotation directory\n",
    "        - from Assembly directory: mg.reads.sorted.bam, mg.assembly.merged.fa, mg.reads.sorted.bam.bai\n",
    "        - FeatureCounts: to extract the number of reads per called gene.\n",
    "            - you supply the GFF file, which contains the positions of the genes (or open reading frames), and the .bam file, which contains for all the reads the positions where they map. Feature counts just counts how many reads map on each of the open reading frames.\n",
    "            - output in mg.CDS_counts.tsv contains a line with the cull call to featureCounts and a header line and then one line for all features in the GFF file. As you can see in the header line, the first field in every line gives the actual feature and the last column contains the number of reads per feature.\n",
    "        - Running featureCounts for KO profiles \n",
    "            - featureCounts can also get numbers of reads per level of the same annotations, e.g. KOs. To run the calculation for the open reading frames that were annotated with a KO, we first need to filter the GFF file, because featureCounts is a bit unflexible with its input:\n",
    "                - grep \"kegg_ko=\" /zfs/omics/projects/metatools/SANDBOX/Metagenomics101/EXAMPLE_DATA/PROFILES/annotation_CDS_RNA_hmms.gff >> annotation_KEGG.gff\n",
    "            - Files with counts generated in this way can be used for differential abundance analysis, e.g. by DESeq2. Here is an example script to summarize featureCounts outputs from several assembled samples.\n",
    "                - DESeq2: https://bioconductor.org/packages/devel/bioc/manuals/DESeq2/man/DESeq2.pdf \n",
    "                - script: https://github.com/a-h-b/mump/blob/main/workflow/scripts/multi_collect_featureCounts.R\n",
    "        - Calculating average depth of coverage values\n",
    "            - When working with functional profiles, sometimes you will be interested in how many copies you had of a gene and how abundant they are. For example, you may have identified a function of interest. With your GFF file, it is quite easy to identify the genes that have this function (see session 07). Now, you could look at the number of reads that map to each of those genes in the featureCounts output from the start of this lesson. But this only tells you how many reads overlapped. If the genes have different lengths, this is not the best measure of their abundance. In this case, you would want to have the average coverage:\n",
    "            - use the same conda environment as before (used for featureCounts): conda activate /zfs/omics/projects/metatools/TOOLS/IMP3/conda/8fab667c26067cc13be382368594bb18\n",
    "            - First, we use a small script to get the length of all contigs. We need this to make a nicely formatted file with all the genes.:\n",
    "                - export PERL5LIB=$CONDA_PREFIX/lib/site_perl/5.26.2\n",
    "                - perl /zfs/omics/projects/metatools/TOOLS/IMP3/workflow/scripts/fastaNamesSizes.pl  /zfs/omics/projects/metatools/SANDBOX/Metagenomics101/EXAMPLE_DATA/PROFILES/mg.assembly.merged.fa > mg.assembly.length.txt\n",
    "                - conda deactivate\n",
    "            - For the next step, you need to use a different conda environment, which includes the bedtools suite. bedtools is good juggling coordinates (positions) on the assemblies.\n",
    "                - bedtools: https://bedtools.readthedocs.io/en/latest/\n",
    "                - conda activate /zfs/omics/projects/metatools/TOOLS/IMP3/conda/76e02d85877600c41ac84cb7bc27a189\n",
    "                - This step will make a histogram file which contains for every gene the number of positions with a given coverage.\n",
    "                    - sortBed -g mg.assembly.length.txt -i /zfs/omics/projects/metatools/SANDBOX/Metagenomics101/EXAMPLE_DATA/PROFILES/annotation_CDS_RNA_hmms.gff |\\\n",
    "                        awk '$4 < $5' |\\\n",
    "                        coverageBed -hist -sorted -g mg.assembly.length.txt -b /zfs/omics/projects/metatools/SANDBOX/Metagenomics101/EXAMPLE_DATA/PROFILES/mg.reads.sorted.bam -a \"stdin\" |\\\n",
    "                        grep -v \"^all\" > helper_bedfile\n",
    "                    - paste <(cat helper_bedfile | cut -f9 | cut -f1 -d \";\" | sed -e \"s/ID=//g\") \\\n",
    "                        <(cut -f10,11,12,13 helper_bedfile) > mg.gene_depth.hist\n",
    "                - A small script combines the histograms to an average value per gene:\n",
    "                    - perl /zfs/omics/projects/metatools/TOOLS/IMP3/workflow/scripts/calcAvgCoverage.pl mg.gene_depth.hist > mg.gene_depth.avg\n",
    "            - In addition to looking at the coverage for specific genes of interest, you can use this file for example for the kind of overvire visualization the IMP3 report gave you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
